{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import torch\n",
    "from nltk.stem import PorterStemmer\n",
    "from transformers import AutoTokenizer ,AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt:str='distilbert-base-uncased'\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)\n",
    "# num_labels:int=2\n",
    "# data_labels=['positive', 'negative']\n",
    "# batch_size:int=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer():\n",
    "    def __init__(self,\n",
    "                model_ckpt = \"distilbert-base-uncased\",\n",
    "                num_labels=2,\n",
    "                batch_size = 64\n",
    "                # data_path='D:\\Codes\\sentiment-fastapi/airline_sentiment_analysis.csv',\n",
    "\n",
    "                ):\n",
    "        \n",
    "        self.model_ckpt=model_ckpt\n",
    "        self.num_labels=num_labels\n",
    "        self.batch_size=batch_size\n",
    "        self.tokenizer=AutoTokenizer.from_pretrained(model_ckpt)\n",
    "        # self.data_path=data_path\n",
    "        self.data_labels=['positive', 'negative']\n",
    "        self.save_path=f\"{model_ckpt}-finetuned-emotion\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data=(pd.read_csv(path, index_col=0, header=[0])).reset_index(drop=True)\n",
    "        data.columns=['label','text']\n",
    "        data=data[['text','label']]\n",
    "        return data\n",
    "\n",
    "    def preprocess_text(self,text):\n",
    "        stemmer = PorterStemmer()\n",
    "        entity_prefixes = ['@']\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            word = word.strip()\n",
    "            if word:\n",
    "                if word[0] not in entity_prefixes:\n",
    "                    word= stemmer.stem(word)\n",
    "                    words.append(word)\n",
    "        sentence=' '.join(words)\n",
    "\n",
    "        # remove stock market tickers\n",
    "        tweet = re.sub(r'\\$\\w*', '', sentence)\n",
    "        # remove twitter abbreviations\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        # only removing the hash # sign from the word\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        return tweet\n",
    "\n",
    "\n",
    "    def split_data(self,data):\n",
    "        train, validate, test = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])\n",
    "        return train, validate, test\n",
    "\n",
    "    def create_dateset(self,train,validate,test):\n",
    "        train_dataset = datasets.Dataset.from_dict(train)\n",
    "        test_dataset = datasets.Dataset.from_dict(test)\n",
    "        validation_dataset=datasets.Dataset.from_dict(validate)\n",
    "        my_dataset_dict = datasets.DatasetDict({\"train\":train_dataset,\"validation\":validation_dataset,\"test\":test_dataset})\n",
    "        return my_dataset_dict\n",
    "\n",
    "    def tokenize(self,batch):\n",
    "        return self.tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "    def compute_metrics(self,pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "    def training(self, \n",
    "                load_path='D:\\Codes\\sentiment-fastapi/airline_sentiment_analysis.csv'\n",
    "                \n",
    "                \n",
    "                \n",
    "                ):\n",
    "\n",
    "\n",
    "        data= self.load_data(path=load_path)\n",
    "        le = LabelEncoder()\n",
    "        data.label=le.fit(data.label).transform(data.label)\n",
    "        data.text = [self.preprocess_text(data.text[i]) for i in range(len(data))]\n",
    "        train, validate, test = self.split_data(data=data)\n",
    "        sentiment=self.create_dateset(train,validate,test)\n",
    "\n",
    "        #tokenize and encode\n",
    "        sentiment_encoded = sentiment.map(self.tokenize, batched=True, batch_size=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = (AutoModelForSequenceClassification\n",
    "                .from_pretrained(self.model_ckpt, num_labels=self.num_labels)\n",
    "                .to(device))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        logging_steps = len(sentiment_encoded[\"train\"]) // self.batch_size\n",
    "        model_name = f\"{self.model_ckpt}-finetuned-emotion\"\n",
    "        training_args = TrainingArguments(output_dir=model_name,\n",
    "                                            num_train_epochs=2,\n",
    "                                            learning_rate=2e-5,\n",
    "                                            per_device_train_batch_size=self.batch_size,\n",
    "                                            per_device_eval_batch_size=self.batch_size,\n",
    "                                            weight_decay=0.01,\n",
    "                                            evaluation_strategy=\"epoch\",\n",
    "                                            disable_tqdm=False,\n",
    "                                            logging_steps=logging_steps,\n",
    "                                            push_to_hub=False, \n",
    "                                            log_level=\"error\")\n",
    "\n",
    "        trainer = Trainer(model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=self.compute_metrics,\n",
    "        train_dataset=sentiment_encoded[\"train\"],\n",
    "        eval_dataset=sentiment_encoded[\"validation\"],\n",
    "        tokenizer=self.tokenizer)\n",
    "\n",
    "        # trainer\n",
    "        trainer.train();\n",
    "        trainer.save_model(self.save_path)\n",
    "        return model\n",
    "    # def train_model(data):\n",
    "\n",
    "    #     sentiment_encoded = data.map(tokenize, batched=True, batch_size=None)\n",
    "        \n",
    "    #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #     model = (AutoModelForSequenceClassification\n",
    "    #                 .from_pretrained(model_ckpt, num_labels=2).to(device))\n",
    "    #     logging_steps = len(data[\"train\"]) // batch_size\n",
    "    #     model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "    #     training_args = TrainingArguments(output_dir=model_name,\n",
    "    #                                         num_train_epochs=2,\n",
    "    #                                         learning_rate=2e-5,\n",
    "    #                                         per_device_train_batch_size=batch_size,\n",
    "    #                                         per_device_eval_batch_size=batch_size,\n",
    "    #                                         weight_decay=0.01,\n",
    "    #                                         evaluation_strategy=\"epoch\",\n",
    "    #                                         disable_tqdm=False,\n",
    "    #                                         logging_steps=logging_steps,\n",
    "    #                                         push_to_hub=False, \n",
    "    #                                         log_level=\"error\")\n",
    "    #     trainer = Trainer(model=model, args=training_args, \n",
    "    #                         # compute_metrics=compute_metrics,\n",
    "    #                         train_dataset=data[\"train\"],\n",
    "    #                         eval_dataset=data[\"validation\"],\n",
    "    #                         tokenizer=tokenizer)\n",
    "\n",
    "    #     trainer.train()\n",
    "    #     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.59ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.83ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.83ba/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 50%|█████     | 109/218 [00:54<00:42,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3219, 'learning_rate': 1.0091743119266055e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "TypeError",
     "evalue": "trainer.compute_metrics() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Codes\\sentiment-fastapi\\trainer.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m obj\u001b[39m=\u001b[39m trainer()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39;49mtraining()\n",
      "\u001b[1;32md:\\Codes\\sentiment-fastapi\\trainer.ipynb Cell 4\u001b[0m in \u001b[0;36mtrainer.training\u001b[1;34m(self, load_path)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m compute_metrics\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m train_dataset\u001b[39m=\u001b[39msentiment_encoded[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m eval_dataset\u001b[39m=\u001b[39msentiment_encoded[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m tokenizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39m# trainer\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain();\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_path)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#X11sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1406\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1408\u001b[0m )\n\u001b[1;32m-> 1409\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1410\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1411\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1412\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1413\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1414\u001b[0m )\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\trainer.py:1743\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1740\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1742\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 1743\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1745\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[0;32m   1746\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   1747\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\trainer.py:1912\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1910\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 1912\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[0;32m   1913\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[0;32m   1915\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\trainer.py:2621\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2618\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2620\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2621\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2622\u001b[0m     eval_dataloader,\n\u001b[0;32m   2623\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2624\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2625\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2626\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2627\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2628\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2629\u001b[0m )\n\u001b[0;32m   2631\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2632\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2633\u001b[0m     speed_metrics(\n\u001b[0;32m   2634\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2638\u001b[0m     )\n\u001b[0;32m   2639\u001b[0m )\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\trainer.py:2903\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2899\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[0;32m   2900\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[0;32m   2901\u001b[0m         )\n\u001b[0;32m   2902\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2903\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[0;32m   2904\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2905\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: trainer.compute_metrics() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "obj= trainer()\n",
    "model=obj.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x000001CFD6C8F520> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.73ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.07ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.81ba/s]\n"
     ]
    }
   ],
   "source": [
    "# #load and transform data\n",
    "# obj = trainer()\n",
    "\n",
    "# data=obj.load_data('D:\\Codes\\sentiment-fastapi/airline_sentiment_analysis.csv')\n",
    "# le = LabelEncoder()\n",
    "# data.label=le.fit(data.label).transform(data.label)\n",
    "# data.text = [obj.preprocess_text(data.text[i]) for i in range(len(data))]\n",
    "# train, validate, test = obj.split_data(data=data)\n",
    "# sentiment=obj.create_dateset(train,validate,test)\n",
    "\n",
    "# #tokenize and encode\n",
    "# sentiment_encoded = sentiment.map(obj.tokenize, batched=True, batch_size=None)\n",
    "\n",
    "# model=obj.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/218 [03:09<?, ?it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.63ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.65ba/s]\n"
     ]
    }
   ],
   "source": [
    "# hide_output\n",
    "# sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# model_ckpt = \"distilbert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# num_labels=2\n",
    "# num_labels=len(np.unique(sentiment_encoded[\"train\"]['label']))\n",
    "# batch_size = 64\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training():\n",
    "#   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#   model = (AutoModelForSequenceClassification\n",
    "#           .from_pretrained(model_ckpt, num_labels=2)\n",
    "#           .to(device))\n",
    "\n",
    "#   logging_steps = len(sentiment_encoded[\"train\"]) // batch_size\n",
    "#   model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "#   training_args = TrainingArguments(output_dir=model_name,\n",
    "#                                     num_train_epochs=2,\n",
    "#                                     learning_rate=2e-5,\n",
    "#                                     per_device_train_batch_size=batch_size,\n",
    "#                                     per_device_eval_batch_size=batch_size,\n",
    "#                                     weight_decay=0.01,\n",
    "#                                     evaluation_strategy=\"epoch\",\n",
    "#                                     disable_tqdm=False,\n",
    "#                                     logging_steps=logging_steps,\n",
    "#                                     push_to_hub=False, \n",
    "#                                     log_level=\"error\")\n",
    "\n",
    "#   trainer = Trainer(model=model, args=training_args, \n",
    "#                   #   compute_metrics=compute_metrics,\n",
    "#                     train_dataset=sentiment_encoded[\"train\"],\n",
    "#                     eval_dataset=sentiment_encoded[\"validation\"],\n",
    "#                     tokenizer=tokenizer)\n",
    "#   trainer\n",
    "#   trainer.train();\n",
    "#   trainer.save_model(model_name)\n",
    "#   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 50%|█████     | 109/218 [00:58<00:46,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2747, 'learning_rate': 1.0091743119266055e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 109/218 [01:04<00:46,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17304207384586334, 'eval_runtime': 5.5121, 'eval_samples_per_second': 418.717, 'eval_steps_per_second': 6.713, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 216/218 [02:03<00:01,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1365, 'learning_rate': 1.8348623853211012e-07, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 218/218 [02:09<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16361425817012787, 'eval_runtime': 5.529, 'eval_samples_per_second': 417.433, 'eval_steps_per_second': 6.692, 'epoch': 2.0}\n",
      "{'train_runtime': 129.236, 'train_samples_per_second': 107.153, 'train_steps_per_second': 1.687, 'train_loss': 0.20459835053583897, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('fapi-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a924f89f863658778a2eaec4295dab149a42a16960c92d5797308b3e3b5ba295"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

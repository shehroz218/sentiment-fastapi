{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import torch\n",
    "from nltk.stem import PorterStemmer\n",
    "from transformers import AutoTokenizer ,AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt:str='distilbert-base-uncased'\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)\n",
    "num_labels:int=2\n",
    "data_labels=['positive', 'negative']\n",
    "batch_size:int=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data=(pd.read_csv(path, index_col=0, header=[0])).reset_index(drop=True)\n",
    "    data.columns=['label','text']\n",
    "    data=data[['text','label']]\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    entity_prefixes = ['@']\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                word= stemmer.stem(word)\n",
    "                words.append(word)\n",
    "    sentence=' '.join(words)\n",
    "\n",
    "    # remove stock market tickers\n",
    "    tweet = re.sub(r'\\$\\w*', '', sentence)\n",
    "    # remove twitter abbreviations\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "    train, validate, test = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])\n",
    "    return train, validate, test\n",
    "\n",
    "def create_dateset(train,validate,test):\n",
    "    train_dataset = datasets.Dataset.from_dict(train)\n",
    "    test_dataset = datasets.Dataset.from_dict(test)\n",
    "    validation_dataset=datasets.Dataset.from_dict(validate)\n",
    "    my_dataset_dict = datasets.DatasetDict({\"train\":train_dataset,\"validation\":validation_dataset,\"test\":test_dataset})\n",
    "    return my_dataset_dict\n",
    "\n",
    "def batch_tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "def train_model(data):\n",
    "\n",
    "    sentiment_encoded = data.map(batch_tokenize, batched=True, batch_size=None)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = (AutoModelForSequenceClassification\n",
    "                .from_pretrained(model_ckpt, num_labels=num_labels).to(device))\n",
    "    logging_steps = len(data[\"train\"]) // batch_size\n",
    "    model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "    training_args = TrainingArguments(output_dir=model_name,\n",
    "                                        num_train_epochs=2,\n",
    "                                        learning_rate=2e-5,\n",
    "                                        per_device_train_batch_size=batch_size,\n",
    "                                        per_device_eval_batch_size=batch_size,\n",
    "                                        weight_decay=0.01,\n",
    "                                        evaluation_strategy=\"epoch\",\n",
    "                                        disable_tqdm=False,\n",
    "                                        logging_steps=logging_steps,\n",
    "                                        push_to_hub=False, \n",
    "                                        log_level=\"error\")\n",
    "    trainer = Trainer(model=model, args=training_args, \n",
    "                        # compute_metrics=compute_metrics,\n",
    "                        train_dataset=data[\"train\"],\n",
    "                        eval_dataset=data[\"validation\"],\n",
    "                        tokenizer=tokenizer)\n",
    "\n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.77ba/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.07ba/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.18ba/s]\n",
      "\n",
      "  0%|          | 0/218 [00:35<?, ?it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.29ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.41ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.47ba/s]\n",
      "d:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Codes\\sentiment-fastapi\\trainer.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#tokenize and encode\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m sentiment_encoded \u001b[39m=\u001b[39m sentiment\u001b[39m.\u001b[39mmap(batch_tokenize, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m=\u001b[39mtrain_model(data\u001b[39m=\u001b[39;49msentiment)\n",
      "\u001b[1;32md:\\Codes\\sentiment-fastapi\\trainer.ipynb Cell 4\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(output_dir\u001b[39m=\u001b[39mmodel_name,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m                                     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                                     learning_rate\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m                                     push_to_hub\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m                                     log_level\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model\u001b[39m=\u001b[39mmodel, args\u001b[39m=\u001b[39mtraining_args, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m                     \u001b[39m# compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m                     train_dataset\u001b[39m=\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m                     eval_dataset\u001b[39m=\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m                     tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Codes/sentiment-fastapi/trainer.ipynb#W2sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1406\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1408\u001b[0m )\n\u001b[1;32m-> 1409\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1410\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1411\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1412\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1413\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1414\u001b[0m )\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\trainer.py:1625\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1624\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1625\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1626\u001b[0m \n\u001b[0;32m   1627\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1629\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\data\\data_collator.py:247\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m--> 247\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[0;32m    248\u001b[0m         features,\n\u001b[0;32m    249\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[0;32m    250\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[0;32m    251\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    252\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_tensors,\n\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    254\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m    255\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Codes\\sentiment-fastapi\\fapi-env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2813\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[0;32m   2812\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 2813\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2814\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2815\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2816\u001b[0m     )\n\u001b[0;32m   2818\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[0;32m   2820\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m required_input:\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']"
     ]
    }
   ],
   "source": [
    "#load and transform data\n",
    "data=load_data('D:\\Codes\\sentiment-fastapi/airline_sentiment_analysis.csv')\n",
    "# data.text = [preprocess_text(data.text[i]) for i in range(len(data))]\n",
    "train, validate, test = split_data(data=data)\n",
    "sentiment=create_dateset(train,validate,test)\n",
    "\n",
    "#tokenize and encode\n",
    "sentiment_encoded = sentiment.map(batch_tokenize, batched=True, batch_size=None)\n",
    "model=train_model(data=sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_encoded[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strip_all_tags(text):\n",
    "#     entity_prefixes = ['@']\n",
    "#     words = []\n",
    "#     for word in text.split():\n",
    "#         word = word.strip()\n",
    "#         if word:\n",
    "#             if word[0] not in entity_prefixes:\n",
    "#                 words.append(word)\n",
    "#     return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.text = [strip_all_tags(data.text[i]) for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('fapi-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a924f89f863658778a2eaec4295dab149a42a16960c92d5797308b3e3b5ba295"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
